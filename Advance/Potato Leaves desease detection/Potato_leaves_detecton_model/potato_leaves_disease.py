# -*- coding: utf-8 -*-
"""Potato_leaves_disease.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1e2tJ3LCLwSUWGueiC8wbK6H94VTzpp1L

# Import Libraries
"""

from google.colab import drive
drive.mount('/content/drive')

import tensorflow as tf
from tensorflow.keras import  models,layers
import matplotlib.pyplot as plt

BATCH_SIZE =32
IMAGE_SIZE = 256
CHANNELS = 3
EPOCHS = 50

"""# Load Data"""

dataset = tf.keras.preprocessing.image_dataset_from_directory(
    "/content/drive/MyDrive/Datasets_ML_AI_DL/PlantVillage",
    shuffle=True,
    batch_size=BATCH_SIZE,
    image_size =(IMAGE_SIZE,IMAGE_SIZE)

)

class_names = dataset.class_names
class_names

plt.figure(figsize=(15,15))
for image_batch , labels_batch in dataset.take(1):
  print(image_batch.shape)
  print(labels_batch.numpy())
  for i in range(12):
    ax = plt.subplot(3,4,i+1)
    plt.imshow(image_batch[i].numpy().astype("uint8"))
    plt.title(class_names[labels_batch[i]])
    plt.axis("off")

#80% ==> Training
#20% ==> test
  #10% ==> validation
  #10% ==> Test

def get_dataset_partitions_tf(ds, train_split=0.8,val_split=0.1,test_splt=0.1,shuffle=True,shuffle_size=10000 ):
  assert (train_split+test_splt+val_split) == 1
  ds_size = len(ds)
  if shuffle:
    ds=ds.shuffle(shuffle_size,seed=12)

  train_size= int(train_split*ds_size)
  val_size=int(val_split*ds_size)
  train_ds = ds.take(train_size)
  val_ds = ds.skip(train_size).take(val_size)
  test_ds = ds.skip(train_size).skip(val_size)

  return train_ds, val_ds,test_ds

train_ds, val_ds,test_ds = get_dataset_partitions_tf(dataset)
len(train_ds)
len(val_ds)
len(test_ds)

train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)
val_ds = val_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)
test_ds = test_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)

for image_batch,labels_batch in dataset.take(1):
  print(image_batch[0].numpy()/255)

resize_and_rescale = tf.keras.Sequential([
              layers.experimental.preprocessing.Resizing(IMAGE_SIZE,IMAGE_SIZE),
              layers.experimental.preprocessing.Rescaling(1.0/255)
])
data_augmentation = tf.keras.Sequential([
              layers.experimental.preprocessing.RandomFlip("horizontal_and_vertical"),
              layers.experimental.preprocessing.RandomRotation(0.2),
])

input_shape = (BATCH_SIZE,IMAGE_SIZE,IMAGE_SIZE,CHANNELS)
n_classes =3

CNNmodel = models.Sequential([
                   resize_and_rescale,
                   data_augmentation,
                   layers.Conv2D(32,kernel_size=(3,3),activation='relu',input_shape=input_shape),
                   layers.MaxPooling2D((2,2)),
                   layers.Conv2D(64,kernel_size=(3,3),activation='relu'),
                   layers.MaxPooling2D((2,2)),
                   layers.Conv2D(64,kernel_size=(3,3),activation='relu'),
                   layers.MaxPooling2D((2,2)),
                   layers.Conv2D(64,(3,3),activation='relu'),
                   layers.MaxPooling2D((2,2)),
                   layers.Conv2D(64,(3,3),activation='relu'),
                   layers.MaxPooling2D((2,2)),
                   layers.Conv2D(64,(3,3),activation='relu'),
                   layers.MaxPooling2D((2,2)),   
                               
                   layers.Flatten(),
                   layers.Dense(64,activation="relu"),
                   layers.Dense(n_classes,activation='softmax'),                 

])
#print(input_shape)
CNNmodel.build(input_shape=input_shape)

CNNmodel.summary()

CNNmodel.compile(
    optimizer = 'adam',
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
    metrics = ['accuracy']
)

history = CNNmodel.fit(
    train_ds,
    batch_size = BATCH_SIZE,
    validation_data = val_ds,
    verbose = 1,
    epochs = EPOCHS 
)

scores = CNNmodel.evaluate(test_ds)
print(scores)

history.history.keys()

acc = history.history['accuracy']
val_acc =history.history['val_accuracy']
val_loss = history.history['val_loss']
loss =history.history['loss']

import matplotlib.pyplot as plt
plt.figure(figsize=(8,8))
plt.subplot(1,2,1)
plt.plot(range(EPOCHS),acc, label='Training Accuracy')
plt.plot(range(EPOCHS),val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1,2,2)
plt.plot(range(EPOCHS),loss, label='Training Loss')
plt.plot(range(EPOCHS),val_loss, label='Validation Loss')
plt.legend(loc='upper left')
plt.title('Training and validation loss')
plt.show()

"""# Actual vs Predict"""

import numpy as np
for image_batch,labels_batch in test_ds.take(1):
  first_image = image_batch[0].numpy().astype('uint8')
  #plt.imshow(first_image)
  
  plt.figure(figsize=(18,18))
  for i in range(9):
    plt.subplot(3,3,i+1)
    prediction_value = CNNmodel.predict(image_batch)
    prediction_value_name=class_names[np.argmax(prediction_value[i])]
    actual_value_name = class_names[labels_batch[i].numpy()]
    confidence = (round(100*(np.max(image_batch[i])),2)/256)
    confidence_str = str(confidence)
   # print(confidence_str)
   # print("First image to predict")
   # print("Actual value:::"+actual_value_name)
   # print("Prediction value::"+prediction_value_name)
    first_image = image_batch[i].numpy().astype('uint8')
    plt.imshow(first_image)
    plt.title("Actual:"+(actual_value_name)+"\nPredicted:"+(prediction_value_name)+"\nAcc Confidence:"+(confidence_str)+"%")
    #plt.title()
    plt.axis("off")